{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:27:03.753963Z",
     "start_time": "2025-02-15T21:27:03.462458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install pandas numpy matplotlib seaborn networkx scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92393dc",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89401dc4ecddf320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:28:38.896456Z",
     "start_time": "2025-02-15T21:28:38.413621Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c398de3e70225d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:28:40.426630Z",
     "start_time": "2025-02-15T21:28:40.153546Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('lol_ranked_games.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b579bcdddd13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:17:09.778010Z",
     "start_time": "2025-02-15T22:17:09.768092Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total rows:\", df.shape[0])\n",
    "print(\"Total columns:\", df.shape[1])\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52e847854e3353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:29:20.948610Z",
     "start_time": "2025-02-15T21:29:20.943140Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Missing Data: \\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2317bbd470266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:11:46.637870Z",
     "start_time": "2025-02-15T22:11:46.516483Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff4008f38173ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:30:43.491869Z",
     "start_time": "2025-02-15T21:30:41.255559Z"
    }
   },
   "outputs": [],
   "source": [
    "df.hist(bins=30, figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fce33f18818d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:33:17.204212Z",
     "start_time": "2025-02-15T21:33:12.387698Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_columns = ['goldDiff', 'expDiff', 'kills', 'deaths', 'assists']\n",
    "sns.pairplot(df[selected_columns])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b26eff45b2f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T21:34:33.036657Z",
     "start_time": "2025-02-15T21:34:30.509945Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 35))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d0dc79090ea01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:17:48.057Z",
     "start_time": "2025-02-15T22:17:46.988601Z"
    }
   },
   "outputs": [],
   "source": [
    "winning_correlation = df.corr()['hasWon'].to_frame().T\n",
    "plt.subplots(figsize=(20, 2))\n",
    "sns.heatmap(winning_correlation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883641192cf7fc0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:33:12.383497Z",
     "start_time": "2025-02-15T22:33:12.127766Z"
    }
   },
   "outputs": [],
   "source": [
    "df_f10 = df[df['frame'] == 10]\n",
    "winning_correlation = df_f10.corr()['hasWon'].to_frame().T\n",
    "plt.subplots(figsize=(20, 2))\n",
    "sns.heatmap(winning_correlation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a698fa615db0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:39:02.713297Z",
     "start_time": "2025-02-15T22:39:02.710205Z"
    }
   },
   "outputs": [],
   "source": [
    "high_impact_columns = ['goldDiff', 'expDiff', 'champLevelDiff', 'kills', 'deaths', 'assists', 'isFirstTower', 'isFirstBlood']\n",
    "df_high_impact = df[high_impact_columns]\n",
    "print(df_high_impact.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803319bf24cc418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T23:51:24.089810Z",
     "start_time": "2025-02-15T23:51:23.705299Z"
    }
   },
   "outputs": [],
   "source": [
    "#DATA SETUP\n",
    "\n",
    "# Filter for frame=10\n",
    "df_frame_10 = df[df['frame'] == 10].copy()\n",
    "\n",
    "# Create derived columns\n",
    "df_frame_10['kda'] = df_frame_10['kills'] + (df_frame_10['assists'] // 2) - df_frame_10['deaths']\n",
    "df_frame_10['wardsDiff'] = df_frame_10['wardsPlaced'] - df_frame_10['wardsLost']\n",
    "\n",
    "drake_columns_killed = ['killedFireDrake', 'killedWaterDrake', 'killedAirDrake', 'killedEarthDrake']\n",
    "drake_columns_lost = ['lostFireDrake', 'lostWaterDrake', 'lostAirDrake', 'lostEarthDrake']\n",
    "df_frame_10['drakeDiff'] = df[drake_columns_killed].sum(axis=1) - df[drake_columns_lost].sum(axis=1)\n",
    "\n",
    "turrets_destroyed = ['destroyedTopOuterTurret', 'destroyedMidOuterTurret', 'destroyedBotOuterTurret']\n",
    "turret_lost = ['lostTopOuterTurret', 'lostMidOuterTurret', 'lostBotOuterTurret']\n",
    "df_frame_10['laneProgression'] = df[turrets_destroyed].sum(axis=1) - df[turret_lost].sum(axis=1)\n",
    "\n",
    "# Discretize data\n",
    "df_frame_10['goldDiff'] = (df_frame_10['goldDiff'] > 0).astype(int)\n",
    "df_frame_10['expDiff'] = (df_frame_10['expDiff'] > 0).astype(int)\n",
    "df_frame_10['wardsDiff'] = (df_frame_10['wardsDiff'] > 0).astype(int)\n",
    "df_frame_10['drakeDiff'] = (df_frame_10['drakeDiff'] > 0).astype(int)\n",
    "df_frame_10['kda'] = (df_frame_10['kda'] > 1).astype(int)\n",
    "df_frame_10['killedRiftHerald'] = (df_frame_10['killedRiftHerald'] > 0).astype(int)\n",
    "df_frame_10['laneProgression'] = (df_frame_10['laneProgression'] > 0).astype(int)\n",
    "\n",
    "# Select relevant columns\n",
    "df_bn = df_frame_10[['hasWon', 'goldDiff', 'expDiff', 'kda', 'wardsDiff', 'isFirstBlood', 'isFirstTower',\n",
    "                     'killedRiftHerald', 'drakeDiff', 'laneProgression']]\n",
    "display(df_bn.head())\n",
    "\n",
    "# Define Bayesian network edges manually based on provided design with separate goldDiff and expDiff\n",
    "edges = [\n",
    "    ('kda', 'goldDiff'),\n",
    "    ('kda', 'expDiff'),\n",
    "    ('isFirstBlood', 'kda'),\n",
    "    ('drakeDiff', 'kda'),\n",
    "    ('wardsDiff', 'drakeDiff'),\n",
    "    ('wardsDiff', 'isFirstBlood'),\n",
    "    ('wardsDiff', 'kda'),\n",
    "    ('isFirstTower', 'laneProgression'),\n",
    "    ('laneProgression', 'hasWon'),\n",
    "    ('killedRiftHerald', 'laneProgression'),\n",
    "    ('killedRiftHerald', 'isFirstTower'),\n",
    "    ('drakeDiff', 'kda'),\n",
    "    ('isFirstTower', 'kda'),\n",
    "    ('goldDiff', 'hasWon'),\n",
    "    ('expDiff', 'hasWon')\n",
    "]\n",
    "\n",
    "# Visualize the network\n",
    "plt.figure(figsize=(12, 8))\n",
    "G = nx.DiGraph(edges)\n",
    "nx.draw(G, with_labels=True, node_size=3000, node_color='lightblue', font_size=15, font_weight='bold')\n",
    "plt.title('Bayesian Network for LoL Data with Separate goldDiff and expDiff Nodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db2038",
   "metadata": {},
   "source": [
    "Bayesian Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba547d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SPLITTING AND CPTS\n",
    "\n",
    "target = 'hasWon'\n",
    "train_data, test_data = train_test_split(df_bn, test_size=0.25, random_state=42)\n",
    "print(f\"Training Set Size: {len(train_data)}, Testing Set Size: {len(test_data)}\")\n",
    "\n",
    "# Calculating CPTs using Maximum Likelihood Estimation(MLE)\n",
    "P_hasWon_given_gd_ed_lp = train_data.groupby(['goldDiff', 'expDiff', 'laneProgression'])['hasWon'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_lp_given_ft_rh = train_data.groupby(['isFirstTower', 'killedRiftHerald'])['laneProgression'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_gd_given_k = train_data.groupby('kda')['goldDiff'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_ed_given_k = train_data.groupby('kda')['expDiff'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_k_given_fb_dd_wd_ft = train_data.groupby(['isFirstBlood', 'drakeDiff', 'wardsDiff', 'isFirstTower'])['kda'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_dd_given_wd = train_data.groupby('wardsDiff')['drakeDiff'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_fb_given_wd = train_data.groupby('wardsDiff')['isFirstBlood'].value_counts(normalize=True).unstack().fillna(0)\n",
    "P_ft_given_rh = train_data.groupby('killedRiftHerald')['isFirstTower'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Compute marginal probabilities for nodes without parents\n",
    "P_wd = train_data['wardsDiff'].value_counts(normalize=True).to_dict()\n",
    "P_rh = train_data['killedRiftHerald'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "# DATA VISUALIZING\n",
    "display(P_hasWon_given_gd_ed_lp)\n",
    "display(P_lp_given_ft_rh)\n",
    "display(P_ed_given_k)\n",
    "display(P_k_given_fb_dd_wd_ft)\n",
    "display(P_dd_given_wd)\n",
    "display(P_fb_given_wd)\n",
    "display(P_ft_given_rh)\n",
    "display(P_wd)\n",
    "display(P_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRAINING\n",
    "\n",
    "def predict_win_probability(evidence):\n",
    "    \"\"\"\n",
    "    Computes P(hasWon | evidence) using the trained Bayesian Network.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract evidence values\n",
    "        gd, ed, lp = evidence['goldDiff'], evidence['expDiff'], evidence['laneProgression']\n",
    "\n",
    "        # Compute probability of winning and losing\n",
    "        P_win = P_hasWon_given_gd_ed_lp.loc[gd, ed, lp].get(1, 1e-6)  # P(hasWon=1 | gd, ed, lp)\n",
    "        P_lose = P_hasWon_given_gd_ed_lp.loc[gd, ed, lp].get(0, 1e-6)  # P(hasWon=0 | gd, ed, lp)\n",
    "\n",
    "        # Normalize probabilities\n",
    "        total = P_win + P_lose\n",
    "        P_win /= total\n",
    "        P_lose /= total\n",
    "\n",
    "        return 1 if P_win > P_lose else 0  # Return predicted class\n",
    "    \n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key: {e}, using default probability\")\n",
    "        return np.random.choice([0, 1])  # Randomly assign if missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TESTING AND EVALUATING\n",
    "\n",
    "# Testing data\n",
    "test_data['predicted_hasWon'] = test_data.apply(lambda row: predict_win_probability(row), axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (test_data['hasWon'] == test_data['predicted_hasWon']).mean()\n",
    "print(f\"Model Accuracy: {accuracy*100:.2f}{'%'}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_data['hasWon'], test_data['predicted_hasWon'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Lose\", \"Win\"], yticklabels=[\"Lose\", \"Win\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed3372",
   "metadata": {},
   "source": [
    "HMM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba93b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('lol_ranked_games.csv')\n",
    "df_hmm = df.copy()\n",
    "\n",
    "# Derive new features\n",
    "df_hmm['kda'] = df_hmm['kills'] + (df_hmm['assists'] // 2) - df_hmm['deaths']\n",
    "df_hmm['wardsDiff'] = df_hmm['wardsPlaced'] - df_hmm['wardsLost']\n",
    "drake_columns_killed = ['killedFireDrake', 'killedWaterDrake', 'killedAirDrake', 'killedEarthDrake']\n",
    "drake_columns_lost = ['lostFireDrake', 'lostWaterDrake', 'lostAirDrake', 'lostEarthDrake']\n",
    "df_hmm['drakeDiff'] = df_hmm[drake_columns_killed].sum(axis=1) - df_hmm[drake_columns_lost].sum(axis=1)\n",
    "\n",
    "# Include all turrets for lane progression\n",
    "turrets_destroyed = [\n",
    "    'destroyedTopOuterTurret', 'destroyedMidOuterTurret', 'destroyedBotOuterTurret',\n",
    "    'destroyedTopInnerTurret', 'destroyedMidInnerTurret', 'destroyedBotInnerTurret',\n",
    "    'destroyedTopInhibTurret', 'destroyedMidInhibTurret', 'destroyedBotInhibTurret',\n",
    "    'destroyedTopNexusTurret', 'destroyedBotNexusTurret'\n",
    "]\n",
    "turrets_lost = [\n",
    "    'lostTopOuterTurret', 'lostMidOuterTurret', 'lostBotOuterTurret',\n",
    "    'lostTopInnerTurret', 'lostMidInnerTurret', 'lostBotInnerTurret',\n",
    "    'lostTopInhibTurret', 'lostMidInhibTurret', 'lostBotInhibTurret',\n",
    "    'lostTopNexusTurret', 'lostBotNexusTurret'\n",
    "]\n",
    "turrets_destroyed = [col for col in turrets_destroyed if col in df_hmm.columns]\n",
    "turrets_lost = [col for col in turrets_lost if col in df_hmm.columns]\n",
    "df_hmm['laneProgression'] = df_hmm[turrets_destroyed].sum(axis=1) - df_hmm[turrets_lost].sum(axis=1)\n",
    "\n",
    "# Discretize features into binary (0/1)\n",
    "df_hmm['goldDiff'] = (df_hmm['goldDiff'] > 0).astype(int)\n",
    "df_hmm['expDiff'] = (df_hmm['expDiff'] > 0).astype(int)\n",
    "df_hmm['wardsDiff'] = (df_hmm['wardsDiff'] > 0).astype(int)\n",
    "df_hmm['drakeDiff'] = (df_hmm['drakeDiff'] > 0).astype(int)\n",
    "df_hmm['kda'] = (df_hmm['kda'] > 1).astype(int)\n",
    "df_hmm['killedRiftHerald'] = (df_hmm['killedRiftHerald'] > 0).astype(int) if 'killedRiftHerald' in df_hmm.columns else 0\n",
    "df_hmm['laneProgression'] = (df_hmm['laneProgression'] > 0).astype(int)\n",
    "df_hmm['killedBaronNashor'] = (df_hmm['killedBaronNashor'] > 0).astype(int) if 'killedBaronNashor' in df_hmm.columns else 0\n",
    "df_hmm['lostBaronNashor'] = (df_hmm['lostBaronNashor'] > 0).astype(int) if 'lostBaronNashor' in df_hmm.columns else 0\n",
    "\n",
    "# Define hidden states\n",
    "hidden_states = ['Disadvantaged', 'Even', 'Advantageous']\n",
    "n_states = len(hidden_states)\n",
    "state_map = {state: i for i, state in enumerate(hidden_states)}\n",
    "\n",
    "# Assign initial hidden states with Baron condition\n",
    "states = np.full(len(df_hmm), 'Even', dtype=object)\n",
    "states[(df_hmm['goldDiff'] == 1) & (df_hmm['expDiff'] == 1) & (df_hmm.get('lostBaronNashor', 0) == 0)] = 'Advantageous'\n",
    "states[(df_hmm['goldDiff'] == 0) & (df_hmm['expDiff'] == 0) | (df_hmm.get('lostBaronNashor', 0) > 0)] = 'Disadvantaged'\n",
    "df_hmm['hidden_state'] = states\n",
    "\n",
    "# Function to filter frames\n",
    "def filter_frames(sequences, start_frame=10, end_frame=None):\n",
    "    filtered_sequences = []\n",
    "    for game in sequences:\n",
    "        # Filter rows within the frame range\n",
    "        if end_frame is not None:\n",
    "            filtered_game = game[(game['frame'] >= start_frame) & (game['frame'] <= end_frame)]\n",
    "        else:\n",
    "            filtered_game = game[game['frame'] >= start_frame]\n",
    "        # Only include non-empty games\n",
    "        if not filtered_game.empty:\n",
    "            filtered_sequences.append(filtered_game)\n",
    "    return filtered_sequences\n",
    "\n",
    "# Prepare sequences for different frame ranges\n",
    "grouped = df_hmm.groupby('gameId')\n",
    "sequences = [group.sort_values('frame') for _, group in grouped]\n",
    "sequences_all = filter_frames(sequences)\n",
    "sequences_10_20 = filter_frames(sequences, 10, 20)\n",
    "sequences_10_30 = filter_frames(sequences, 10, 30)\n",
    "\n",
    "# Observed features including Baron\n",
    "observed_features = ['goldDiff', 'expDiff', 'kda', 'wardsDiff', \n",
    "                     'isFirstBlood', 'isFirstTower', 'killedRiftHerald', \n",
    "                     'drakeDiff', 'laneProgression']\n",
    "if 'killedBaronNashor' in df_hmm.columns:\n",
    "    observed_features.append('killedBaronNashor')\n",
    "if 'lostBaronNashor' in df_hmm.columns:\n",
    "    observed_features.append('lostBaronNashor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ba67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAINING\n",
    "\n",
    "def train_hmm(sequences):\n",
    "    train_games, _ = train_test_split(sequences, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # Compute Transition Probabilities\n",
    "    transition_matrix = np.zeros((n_states, n_states))\n",
    "    for game in train_games:\n",
    "        game_states = game['hidden_state'].values\n",
    "        for prev, curr in zip(game_states[:-1], game_states[1:]):\n",
    "            transition_matrix[state_map[prev], state_map[curr]] += 1\n",
    "    transition_matrix = (transition_matrix + 1e-6) / (transition_matrix.sum(axis=1, keepdims=True) + 1e-6 * n_states)\n",
    "    transition_matrix = np.log(transition_matrix)\n",
    "\n",
    "    # Compute Emission Probabilities\n",
    "    emission_matrix = np.zeros((n_states, len(observed_features)))\n",
    "    for state in hidden_states:\n",
    "        state_data = pd.concat(train_games)[pd.concat(train_games)['hidden_state'] == state][observed_features]\n",
    "        if not state_data.empty:\n",
    "            emission_matrix[state_map[state], :] = state_data.mean().values\n",
    "    emission_matrix = np.log(emission_matrix + 1e-6)\n",
    "\n",
    "    # Viterbi Algorithm\n",
    "    def viterbi(observations, transition_matrix, emission_matrix, start_probs):\n",
    "        T = len(observations)\n",
    "        N = n_states\n",
    "        V = np.zeros((T, N))\n",
    "        backpointer = np.zeros((T, N), dtype=int)\n",
    "        V[0, :] = np.log(start_probs) + np.sum(observations[0] * emission_matrix, axis=1)\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                prob = V[t-1, :] + transition_matrix[:, j]\n",
    "                V[t, j] = np.max(prob) + np.sum(observations[t] * emission_matrix[j])\n",
    "                backpointer[t, j] = np.argmax(prob)\n",
    "        best_path_prob = np.max(V[-1, :])\n",
    "        best_path_pointer = np.argmax(V[-1, :])\n",
    "        best_path = [best_path_pointer]\n",
    "        for t in range(T-1, 0, -1):\n",
    "            best_path_pointer = backpointer[t, best_path_pointer]\n",
    "            best_path.append(best_path_pointer)\n",
    "        best_path = best_path[::-1]\n",
    "        return [hidden_states[state_idx] for state_idx in best_path]\n",
    "\n",
    "    # Compute P(hasWon | final_state)\n",
    "    final_state_counts = {state: {'win': 0, 'lose': 0} for state in hidden_states}\n",
    "    for game in train_games:\n",
    "        obs = game[observed_features].values\n",
    "        start_probs = np.ones(n_states) / n_states\n",
    "        state_sequence = viterbi(obs, transition_matrix, emission_matrix, start_probs)\n",
    "        final_state = state_sequence[-1]\n",
    "        has_won = game['hasWon'].iloc[-1]\n",
    "        final_state_counts[final_state]['win' if has_won else 'lose'] += 1\n",
    "\n",
    "    win_probs = {}\n",
    "    for state in hidden_states:\n",
    "        wins = final_state_counts[state]['win']\n",
    "        total = wins + final_state_counts[state]['lose']\n",
    "        win_probs[state] = wins / total if total > 0 else 0.5\n",
    "\n",
    "    return transition_matrix, emission_matrix, viterbi, win_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TESTING\n",
    "\n",
    "def test_hmm(sequences, transition_matrix, emission_matrix, viterbi, win_probs):\n",
    "    _, test_games = train_test_split(sequences, test_size=0.25, random_state=42)\n",
    "    \n",
    "    predictions = []\n",
    "    true_outcomes = []\n",
    "    final_states = []\n",
    "    start_probs = np.ones(n_states) / n_states\n",
    "    \n",
    "    for game in test_games:\n",
    "        obs = game[observed_features].values\n",
    "        state_sequence = viterbi(obs, transition_matrix, emission_matrix, start_probs)\n",
    "        final_state = state_sequence[-1]\n",
    "        win_prob = win_probs[final_state]\n",
    "        predicted_has_won = 1 if win_prob > 0.5 else 0\n",
    "        predictions.append(predicted_has_won)\n",
    "        true_outcomes.append(game['hasWon'].iloc[-1])\n",
    "        final_states.append(final_state)\n",
    "\n",
    "    return predictions, true_outcomes, final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATING\n",
    "\n",
    "def evaluate_hmm(predictions, true_outcomes, final_states, name):\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(true_outcomes))\n",
    "    print(f\"{name} HMM Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{name} Win probabilities:\", win_probs)\n",
    "    print(f\"{name} Final state distribution:\\n\", pd.Series(final_states).value_counts())\n",
    "    print(f\"{name} Test set wins:\", sum(true_outcomes), \"losses:\", len(true_outcomes) - sum(true_outcomes))\n",
    "    print(f\"{name} Final state vs. Actual outcomes:\")\n",
    "    for state in set(final_states):\n",
    "        state_wins = sum(1 for s, o in zip(final_states, true_outcomes) if s == state and o == 1)\n",
    "        state_losses = sum(1 for s, o in zip(final_states, true_outcomes) if s == state and o == 0)\n",
    "        print(f\"{name} {state}: Wins={state_wins}, Losses={state_losses}\")\n",
    "\n",
    "    cm = confusion_matrix(true_outcomes, predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Lose\", \"Win\"], yticklabels=[\"Lose\", \"Win\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{name} Confusion Matrix (Threshold 0.5)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTING WITH DIFFERENT RANGES OF FRAMES\n",
    "\n",
    "# Train and evaluate for different frame ranges\n",
    "for name, sequences in [(\"All Frames\", sequences_all), (\"Frames 10-20\", sequences_10_20), (\"Frames 10-30\", sequences_10_30)]:\n",
    "    transition_matrix, emission_matrix, viterbi, win_probs = train_hmm(sequences)\n",
    "    predictions, true_outcomes, final_states = test_hmm(sequences, transition_matrix, emission_matrix, viterbi, win_probs)\n",
    "    evaluate_hmm(predictions, true_outcomes, final_states, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75872e",
   "metadata": {},
   "source": [
    "Other models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7cfc8d68d10e87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:39:30.892272Z",
     "start_time": "2025-03-16T04:38:26.701214Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "###################################\n",
    "#         Configuration           #\n",
    "###################################\n",
    "DATA_PATH = \"./data/lol_ranked_games.csv\"  # Adjust if needed\n",
    "TARGET_COL = \"hasWon\"  # Target column for classification\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 1337\n",
    "\n",
    "# Set seeds for reproducibility (PyTorch, numpy, python.random)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# If you have other random libs, set them similarly.\n",
    "\n",
    "###################################\n",
    "#        1. Load & Split Data     #\n",
    "###################################\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "print(\"\\n[Label Distribution]\")\n",
    "print(df[TARGET_COL].value_counts())\n",
    "print(\"\\n[Unique Labels]\")\n",
    "print(df[TARGET_COL].unique())\n",
    "\n",
    "# Ensure labels are 0 or 1. If not, map them.\n",
    "# Example if your dataset uses \"Blue\" / \"Red\" or 1 / 2:\n",
    "# df[TARGET_COL] = df[TARGET_COL].map({\"Blue\": 0, \"Red\": 1})\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(\"\\nData info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "missing_vals = df.isna().sum()\n",
    "print(\"\\nMissing values in each column:\\n\", missing_vals)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {X_train.shape[0]} | Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: check class distribution in training set\n",
    "print(\"\\nTraining set label distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "##################################\n",
    "#    2. Scale/Normalize Inputs   #\n",
    "##################################\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "X_test_torch  = torch.tensor(X_test_scaled,  dtype=torch.float32).to(device)\n",
    "y_test_torch  = torch.tensor(y_test.values,  dtype=torch.long).to(device)\n",
    "\n",
    "##################################\n",
    "#  3. Define AdvancedNN Model    #\n",
    "##################################\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=128, hidden2=64, hidden3=32, output_dim=2, dropout_p=0.2):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        # Layer 1\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Layer 2\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Layer 3\n",
    "        self.fc3 = nn.Linear(hidden2, hidden3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden3)\n",
    "        self.dropout3 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(hidden3, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "##############################\n",
    "# 4. Initialize & Train NN   #\n",
    "##############################\n",
    "model = AdvancedNN(input_dim=X_train_torch.shape[1], dropout_p=0.2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # smaller LR\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "print(\"\\n[Training Advanced Neural Network]\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train_torch.size(0))\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = int(np.ceil(X_train_torch.size(0) / batch_size))\n",
    "\n",
    "    batch_iter = tqdm(range(0, X_train_torch.size(0), batch_size),\n",
    "                      desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                      leave=False)\n",
    "\n",
    "    for i in batch_iter:\n",
    "        indices = permutation[i : i + batch_size]\n",
    "        batch_x, batch_y = X_train_torch[indices], y_train_torch[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Evaluate on test set each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_torch)\n",
    "        _, test_preds = torch.max(test_outputs, 1)\n",
    "        test_acc = (test_preds == y_test_torch).float().mean().item()\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# Final NN Accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_outputs = model(X_test_torch)\n",
    "    _, final_preds = torch.max(final_outputs, 1)\n",
    "nn_acc = (final_preds == y_test_torch).float().mean().item()\n",
    "print(f\"[Advanced Neural Network] Final Test Accuracy: {nn_acc:.4f}\")\n",
    "\n",
    "##############################\n",
    "#  5. Sanity Check Overfit   #\n",
    "##############################\n",
    "# (Optional) Test if the network can overfit a small subset\n",
    "# If it can't get near 100% on the training subset, there's definitely something wrong.\n",
    "subset_size = 200  # or smaller if your data is huge\n",
    "X_small = X_train_torch[:subset_size]\n",
    "y_small = y_train_torch[:subset_size]\n",
    "\n",
    "tmp_model = AdvancedNN(input_dim=X_small.shape[1], dropout_p=0.0).to(device)\n",
    "tmp_optimizer = optim.Adam(tmp_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    tmp_model.train()\n",
    "    tmp_optimizer.zero_grad()\n",
    "    outputs = tmp_model(X_small)\n",
    "    loss = criterion(outputs, y_small)\n",
    "    loss.backward()\n",
    "    tmp_optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = tmp_model(X_small)\n",
    "        _, pred_classes = torch.max(preds, 1)\n",
    "        train_acc = (pred_classes == y_small).float().mean().item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"[Overfit Check] Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {train_acc:.4f}\")\n",
    "\n",
    "# If this \"overfit check\" doesn't reach 95%+ accuracy on a tiny subset,\n",
    "# there's likely a data or code bug.\n",
    "\n",
    "##############################\n",
    "#    6. Save the Model       #\n",
    "##############################\n",
    "torch.save(model.state_dict(), \"advanced_nn_checkpoint.pth\")\n",
    "print(f\"\\nFinal Test Accuracy (full) = {nn_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0cdcf73e9432f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:41:33.335546Z",
     "start_time": "2025-03-16T04:41:07.079157Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1) XGBoost for GPU\n",
    "import xgboost as xgb\n",
    "\n",
    "# 2) Random Forest (CPU-based from scikit-learn)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 3) PyTorch for Neural Network (GPU if available)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For displaying progress bars\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "###################################\n",
    "#         Configuration           #\n",
    "###################################\n",
    "DATA_PATH = \"./data/lol_ranked_games.csv\"  # Adjust if needed\n",
    "TARGET_COL = \"hasWon\"  # Target column for classification\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 1337\n",
    "\n",
    "###################################\n",
    "#        1. Load & Split Data     #\n",
    "###################################\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]} | Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "##########################################\n",
    "#  2.1 Train XGBoost on GPU w/ tqdm      #\n",
    "##########################################\n",
    "# We'll manually loop through boosting rounds to display a tqdm progress bar.\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"tree_method\": \"gpu_hist\",     # GPU usage\n",
    "    \"predictor\": \"gpu_predictor\",  # GPU usage\n",
    "    \"seed\": RANDOM_SEED\n",
    "}\n",
    "\n",
    "num_boost_round = 100\n",
    "booster = None\n",
    "\n",
    "print(\"\\n[Training XGBoost]\")\n",
    "for i in tqdm(range(num_boost_round), desc=\"XGBoost Rounds\"):\n",
    "    # Train 1 boosting round at a time, continuing on the existing booster\n",
    "    booster = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1,\n",
    "        xgb_model=booster  # continue on previously trained model\n",
    "    )\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_pred_proba = booster.predict(dtest)\n",
    "xgb_pred = [1 if p > 0.5 else 0 for p in xgb_pred_proba]\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "print(f\"[XGBoost] Test Accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "# Save XGBoost model\n",
    "booster.save_model(\"xgboost_gpu.model\")\n",
    "\n",
    "##########################################\n",
    "#  2.2 Train RandomForest w/ tqdm (CPU)  #\n",
    "##########################################\n",
    "# scikit-learn RandomForest doesn't natively show progress. We'll do an incremental approach:\n",
    "trees = 100\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=1,      # start with 1 tree\n",
    "    warm_start=True,     # allow incremental training\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"\\n[Training Random Forest (CPU)]\")\n",
    "for i in tqdm(range(1, trees + 1), desc=\"RandomForest Trees\"):\n",
    "    rf.set_params(n_estimators=i)  # incrementally add trees\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "print(f\"[Random Forest] Test Accuracy: {rf_acc:.4f}\")\n",
    "\n",
    "# Save Random Forest model\n",
    "with open(\"random_forest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#        3. Compare and Print Results    #\n",
    "##########################################\n",
    "print(\"\\n======================================\")\n",
    "print(\"Model Accuracy Comparison:\")\n",
    "print(f\" - XGBoost (GPU):          {xgb_acc:.4f}\")\n",
    "print(f\" - Random Forest (CPU):    {rf_acc:.4f}\")\n",
    "print(f\" - Neural Network (GPU?):  {nn_acc:.4f}\")\n",
    "print(\"======================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895534e0e4eb0b14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:59:18.917007Z",
     "start_time": "2025-03-16T19:59:05.193071Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA Implementation and Enhanced Neural Network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "###################################\n",
    "#         Configuration           #\n",
    "###################################\n",
    "DATA_PATH = \"./data/lol_ranked_games.csv\"\n",
    "TARGET_COL = \"hasWon\"\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 1337\n",
    "PCA_COMPONENTS = 0.95  # Keep 95% of variance\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "###################################\n",
    "#    1. Load & Preprocess Data    #\n",
    "###################################\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "# df = df[df['frame'] == 30]\n",
    "# high_impact_columns = ['hasWon', 'goldDiff', 'expDiff', 'champLevelDiff', 'kills', 'deaths', 'assists', 'isFirstTower', 'isFirstBlood']\n",
    "# df = df[high_impact_columns]\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "print(\"\\n[Label Distribution]\")\n",
    "print(df[TARGET_COL].value_counts())\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Check for missing values\n",
    "missing_vals = df.isna().sum()\n",
    "print(\"\\nMissing values in each column:\\n\", missing_vals)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {X_train.shape[0]} | Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "###################################\n",
    "#    2. Apply PCA Reduction       #\n",
    "###################################\n",
    "# Scale data first\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_SEED)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Print PCA information\n",
    "print(f\"\\nOriginal feature count: {X_train.shape[1]}\")\n",
    "print(f\"PCA reduced feature count: {X_train_pca.shape[1]}\")\n",
    "print(f\"Explained variance ratio: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Convert to torch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_pca, dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "X_test_torch = torch.tensor(X_test_pca, dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "###################################\n",
    "#  3. Enhanced Neural Network     #\n",
    "###################################\n",
    "class EnhancedNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], output_dim=2, dropout_p=0.3):\n",
    "        super(EnhancedNN, self).__init__()\n",
    "\n",
    "        # Create layers dynamically based on hidden_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout_p))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_p))\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        # Residual connections (for layers with matching dimensions)\n",
    "        self.use_residual = len(hidden_dims) > 1\n",
    "        if self.use_residual:\n",
    "            self.residual_layers = nn.ModuleList()\n",
    "            for i in range(len(hidden_dims)-1):\n",
    "                if hidden_dims[i] == hidden_dims[i+1]:\n",
    "                    self.residual_layers.append(nn.Identity())\n",
    "                else:\n",
    "                    self.residual_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_outputs = []\n",
    "\n",
    "        # Process through first layer\n",
    "        x = self.layers[0](x)  # Linear\n",
    "        x = self.layers[1](x)  # BatchNorm\n",
    "        x = self.layers[2](x)  # ReLU\n",
    "        x = self.layers[3](x)  # Dropout\n",
    "        layer_outputs.append(x)\n",
    "\n",
    "        # Process through hidden layers with residual connections\n",
    "        layer_idx = 4\n",
    "        res_idx = 0\n",
    "\n",
    "        while layer_idx < len(self.layers):\n",
    "            identity = layer_outputs[-1]\n",
    "\n",
    "            x = self.layers[layer_idx](x)     # Linear\n",
    "            x = self.layers[layer_idx+1](x)   # BatchNorm\n",
    "\n",
    "            # Add residual connection if dimensions match\n",
    "            if self.use_residual:\n",
    "                x = x + self.residual_layers[res_idx](identity)\n",
    "                res_idx += 1\n",
    "\n",
    "            x = self.layers[layer_idx+2](x)   # ReLU\n",
    "            x = self.layers[layer_idx+3](x)   # Dropout\n",
    "\n",
    "            layer_outputs.append(x)\n",
    "            layer_idx += 4\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "###################################\n",
    "#  4. Training with Improvements  #\n",
    "###################################\n",
    "# Initialize model with improved architecture\n",
    "model = EnhancedNN(\n",
    "    input_dim=X_train_torch.shape[1],\n",
    "    hidden_dims=[256, 128, 64, 32],  # Deeper network\n",
    "    dropout_p=0.3\n",
    ").to(device)\n",
    "\n",
    "# Use weighted loss for imbalanced classes if needed\n",
    "class_counts = y_train.value_counts()\n",
    "if abs(class_counts[0] - class_counts[1]) > 0.1 * len(y_train):\n",
    "    class_weights = torch.tensor([1.0, class_counts[0]/class_counts[1]], dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    print(f\"Using weighted loss with weights: {class_weights.cpu().numpy()}\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler and optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10\n",
    "batch_size = 250\n",
    "early_stopping_patience = 15\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"\\n[Training Enhanced Neural Network with PCA]\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train_torch.size(0))\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    num_batches = int(np.ceil(X_train_torch.size(0) / batch_size))\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    batch_iterator = tqdm(range(0, X_train_torch.size(0), batch_size),\n",
    "                          desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                          leave=False,\n",
    "                          total=num_batches)\n",
    "\n",
    "    for i in batch_iterator:\n",
    "        # Get batch indices\n",
    "        indices = permutation[i:i+batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_x, batch_y = X_train_torch[indices], y_train_torch[indices]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += batch_y.size(0)\n",
    "        correct_train += (predicted == batch_y).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        batch_iterator.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': correct_train / total_train\n",
    "        })\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / num_batches\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Process validation data in batches to avoid memory issues\n",
    "        for i in range(0, X_test_torch.size(0), batch_size):\n",
    "            batch_x = X_test_torch[i:i+batch_size]\n",
    "            batch_y = y_test_torch[i:i+batch_size]\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += batch_y.size(0)\n",
    "            correct_val += (predicted == batch_y).sum().item()\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_loss / (X_test_torch.size(0) // batch_size + 1)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Overfit check as requested\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"[Overfit Check] Epoch {epoch+1}, Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        if train_acc - val_acc > 0.05:\n",
    "            print(f\"⚠️ Potential overfitting detected: Train acc {train_acc:.4f} vs Val acc {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model based on validation loss\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'enhanced_nn_checkpoint.pth')\n",
    "print(\"Model saved to 'enhanced_nn_checkpoint.pth'\")\n",
    "\n",
    "###################################\n",
    "#  5. Evaluate Final Model        #\n",
    "###################################\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions\n",
    "    y_pred = model(X_test_torch)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    y_true = y_test_torch.cpu().numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, predicted)\n",
    "    report = classification_report(y_true, predicted)\n",
    "    conf_matrix = confusion_matrix(y_true, predicted)\n",
    "\n",
    "    print(f\"\\n[Final Model Evaluation]\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Loss', 'Win'],\n",
    "                yticklabels=['Loss', 'Win'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "###################################\n",
    "#  6. Visualize Training History  #\n",
    "###################################\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###################################\n",
    "#  7. Feature Importance Analysis #\n",
    "###################################\n",
    "# Get feature importance from PCA components\n",
    "feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "plt.title('Top 15 Features by PCA Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(importance_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
